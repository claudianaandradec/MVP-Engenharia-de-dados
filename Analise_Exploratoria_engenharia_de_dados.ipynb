{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48eb08e-9da9-4232-8e52-c9973b9ead5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1. IMPORTAR BIBLIOTECAS\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql import Row\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bb72af-2696-4e74-a447-c8aa490f9e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schemas criados (databases)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. CONFIGURAR BANCOS DE DADOS (SCHEMAS)\n",
    "# -----------------------------------------------------------\n",
    "print(\"schemas criados (databases)\")\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296845bb-7000-4a0f-986a-65dc069f55df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books carregado\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3. CARGAS DA CAMADA BRONZE (RAW -> DELTA)\n",
    "# -----------------------------------------------------------\n",
    "# Caminho base (ajuda se precisar mudar o volume depois)\n",
    "base_path = \"/Volumes/workspace/default/books_goodreads\"\n",
    "\n",
    "# --- 3.1 CARREGAR BOOKS.CSV ---\n",
    "print(\"Books carregado\")\n",
    "books_bronze = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True) # Identifica int, double, string automaticamente\n",
    "        .csv(f\"{base_path}/books.csv\")\n",
    ")\n",
    "\n",
    "# Salva sobrescrevendo e permitindo sobrescrita de schema (overwriteSchema é mais seguro que mergeSchema para cargas full)\n",
    "books_bronze.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"bronze.books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d589c288-d4f4-4286-b50d-d69a7849f0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings carregado\n"
     ]
    }
   ],
   "source": [
    "# --- 3.2 CARREGAR RATINGS.CSV ---\n",
    "print(\"Ratings carregado\")\n",
    "ratings_bronze = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(f\"{base_path}/ratings.csv\")\n",
    ")\n",
    "\n",
    "ratings_bronze.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"bronze.ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5602e6-d687-4307-8917-2cb503c4cbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags carregado\n"
     ]
    }
   ],
   "source": [
    "# --- 3.3 CARREGAR TAGS.CSV ---\n",
    "print(\"Tags carregado\")\n",
    "tags_bronze = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True) # Adicionei inferSchema para garantir IDs numéricos\n",
    "        .csv(f\"{base_path}/tags.csv\")\n",
    ")\n",
    "\n",
    "tags_bronze.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"bronze.tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebe7c08-9e49-4bd2-9966-2d10dd4527ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book_Tags carregado\n"
     ]
    }
   ],
   "source": [
    "# --- 3.4 CARREGAR BOOK_TAGS.CSV ---\n",
    "# CORREÇÃO: Caminho do arquivo alterado de tags.csv para book_tags.csv\n",
    "print(\"Book_Tags carregado\")\n",
    "book_tags_bronze = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(f\"{base_path}/book_tags.csv\") \n",
    ")\n",
    "\n",
    "book_tags_bronze.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"bronze.book_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2a8545-7fd1-4973-9f8e-32f7c4ae6673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity carregado\nCarga Bronze concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# --- 3.5 CARREGAR POPULARITY.CSV ---\n",
    "print(\"Popularity carregado\")\n",
    "popularity_bronze = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .option(\"sep\", \",\") \n",
    "        .csv(f\"{base_path}/popularity_id.csv\")\n",
    ")\n",
    "\n",
    "popularity_bronze.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"bronze.popularity\")\n",
    "\n",
    "print(\"Carga Bronze concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fee68c-943d-4439-b87f-03f6e2388262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camada SILVER\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1. IMPORTS E FUNÇÕES AUXILIARES\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql.functions import col, trim, when, lower, current_timestamp\n",
    "# Importe colunas que serão usadas (ex: year se precisar tratar nulos)\n",
    "\n",
    "print(\"Camada SILVER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ce64a4-494a-485f-abf0-5966dca73785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Silver\nTabela Books processada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. TABELA BOOKS (Corrigida com tratamento de erros/dirty data)\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql.functions import col, trim, expr\n",
    "\n",
    "print(\"Books Silver\")\n",
    "\n",
    "books_bronze = spark.table(\"bronze.books\")\n",
    "\n",
    "books_silver = (\n",
    "    books_bronze\n",
    "        .select(\n",
    "            # IDs\n",
    "            col(\"id\").cast(\"int\").alias(\"book_id\"),\n",
    "            col(\"book_id\").cast(\"int\").alias(\"goodreads_book_id\"),\n",
    "            col(\"best_book_id\").cast(\"int\"),\n",
    "            col(\"work_id\").cast(\"int\"),\n",
    "            \n",
    "            # Dados do Livro\n",
    "            col(\"books_count\").cast(\"int\"),\n",
    "            col(\"isbn\"),\n",
    "            col(\"isbn13\").cast(\"string\"),\n",
    "            \n",
    "            trim(col(\"authors\")).alias(\"authors\"),\n",
    "            # try_cast para ano, pois as vezes vem texto sujo\n",
    "            expr(\"try_cast(original_publication_year as int)\").alias(\"publication_year\"),\n",
    "            trim(col(\"original_title\")).alias(\"original_title\"),\n",
    "            trim(col(\"title\")).alias(\"title\"),\n",
    "            col(\"language_code\"),\n",
    "            \n",
    "            # --- AQUI ESTAVA O ERRO ---\n",
    "            # Usamos try_cast para que 'eng' vire NULL em vez de travar o pipeline\n",
    "            expr(\"try_cast(average_rating as double)\").alias(\"average_rating\"),\n",
    "            expr(\"try_cast(ratings_count as int)\").alias(\"ratings_count\")\n",
    "        )\n",
    "        # Removemos linhas que ficaram com ID nulo (sujeira severa)\n",
    "        .filter(col(\"book_id\").isNotNull())\n",
    "        .dropDuplicates([\"book_id\"])\n",
    ")\n",
    "\n",
    "books_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.books\")\n",
    "print(\"Tabela Books processada com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949022b2-540f-41e2-b26a-881d3ce5e42c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings Silver\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3. TABELA RATINGS (Interações)\n",
    "# -----------------------------------------------------------\n",
    "print(\"Ratings Silver\")\n",
    "\n",
    "ratings_bronze = spark.table(\"bronze.ratings\")\n",
    "\n",
    "ratings_silver = (\n",
    "    ratings_bronze\n",
    "        .select(\n",
    "            col(\"book_id\").cast(\"int\"),\n",
    "            col(\"user_id\").cast(\"int\"),\n",
    "            col(\"rating\").cast(\"int\")\n",
    "        )\n",
    "        # Remove duplicatas se o mesmo usuário avaliou o mesmo livro duas vezes (mantém um)\n",
    "        .dropDuplicates([\"book_id\", \"user_id\"])\n",
    ")\n",
    "\n",
    "ratings_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e654f959-e2db-4e2e-92e2-3bfa8adbfd82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando Popularity...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4. TABELA POPULARITY (Auxiliar)\n",
    "# -----------------------------------------------------------\n",
    "print(\"Popularity Silver\")\n",
    "\n",
    "popularity_bronze = spark.table(\"bronze.popularity\")\n",
    "\n",
    "popularity_silver = (\n",
    "    popularity_bronze\n",
    "        .dropDuplicates([\"popularity_id\"])\n",
    "        .withColumn(\"popularity_level\", trim(col(\"popularity_level\")))\n",
    "        .withColumn(\"rule\", trim(col(\"rule\")))\n",
    ")\n",
    "\n",
    "popularity_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.popularity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c671a3d-bd03-4bf6-a2cb-093f7195ba44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags Silver\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 5. TABELA TAGS (Gêneros/Categorias)\n",
    "# -----------------------------------------------------------\n",
    "print(\"Tags Silver\")\n",
    "\n",
    "tags_bronze = spark.table(\"bronze.tags\")\n",
    "\n",
    "tags_silver = (\n",
    "    tags_bronze\n",
    "        .dropDuplicates([\"tag_id\"])\n",
    ")\n",
    "\n",
    "tags_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3264d586-6f73-4289-9266-b320f7756b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book_Tags Silver\nCamada SILVER concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 6. TABELA BOOK_TAGS (Ligação N:N)\n",
    "# -----------------------------------------------------------\n",
    "print(\"Book_Tags Silver\")\n",
    "\n",
    "book_tags_bronze = spark.table(\"bronze.book_tags\") \n",
    "\n",
    "book_tags_silver = (\n",
    "    book_tags_bronze\n",
    "        .select(\n",
    "            col(\"goodreads_book_id\"),\n",
    "            col(\"tag_id\"),\n",
    "            col(\"count\")\n",
    "        )\n",
    "        # Importante: goodreads_book_id NÃO é o mesmo que book_id da tabela books no dataset Kaggle padrão.\n",
    "        .dropDuplicates([\"goodreads_book_id\", \"tag_id\"])\n",
    ")\n",
    "\n",
    "book_tags_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"silver.book_tags\")\n",
    "\n",
    "print(\"Camada SILVER concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf4f793-17bb-4436-9752-e0f30da2aa71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fato Interações Gold\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1. FATO INTERAÇÕES\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "print(\"Fato Interações Gold\")\n",
    "\n",
    "ratings_silver = spark.table(\"silver.ratings\")\n",
    "\n",
    "# A Fato deve conter os IDs para ligar nas dimensões e as métricas\n",
    "fato_interacoes = (\n",
    "    ratings_silver\n",
    "        .select(\n",
    "            col(\"user_id\"),\n",
    "            col(\"book_id\"),\n",
    "            col(\"rating\"),\n",
    "            # Adicionamos uma chave substituta ou usamos a combinação user-book como PK lógica\n",
    "        )\n",
    ")\n",
    "\n",
    "fato_interacoes.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.fato_interacoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb4153c-49d5-4ced-a2ce-56b1279bbe53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão Usuários Gold\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. DIMENSÃO USUÁRIOS\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql.functions import count, avg, round\n",
    "\n",
    "print(\"Dimensão Usuários Gold\")\n",
    "\n",
    "dim_usuarios = (\n",
    "    ratings_silver\n",
    "        .groupBy(\"user_id\")\n",
    "        .agg(\n",
    "            count(\"book_id\").alias(\"total_reviews\"),\n",
    "            round(avg(\"rating\"), 2).alias(\"avg_score_given\")\n",
    "        )\n",
    ")\n",
    "\n",
    "dim_usuarios.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.dim_usuarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92b6ab8-e19d-4b1e-a37f-a0b882693be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3. DATAFRAME AUXILIAR \n",
    "# -----------------------------------------------------------\n",
    "# Calculamos a média real baseada nos dados que temos, não no metadado antigo do CSV\n",
    "df_metrics = (\n",
    "    ratings_silver\n",
    "        .groupBy(\"book_id\")\n",
    "        .agg(\n",
    "            avg(\"rating\").alias(\"calc_avg_rating\"),\n",
    "            count(\"rating\").alias(\"calc_ratings_count\")\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d884ee3-c37c-40c6-be80-45640d05f413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão Livros Gold\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4. DIMENSÃO LIVROS \n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "print(\"Dimensão Livros Gold\")\n",
    "\n",
    "books_silver = spark.table(\"silver.books\")\n",
    "\n",
    "# Join com as métricas calculadas acima\n",
    "dim_books_logic = (\n",
    "    books_silver\n",
    "        .join(df_metrics, on=\"book_id\", how=\"left\")\n",
    "        # Preencher nulos para livros que não tiveram reviews neste dataset\n",
    "        .fillna(0, subset=[\"calc_ratings_count\"]) \n",
    ")\n",
    "\n",
    "# Aplicar Regra de Popularidade\n",
    "dim_livros_gold = (\n",
    "    dim_books_logic\n",
    "        .withColumn(\n",
    "            \"popularity_level\",\n",
    "            when(col(\"calc_ratings_count\") < 100, \"Baixa\") # Ajustei para 100 pois o dataset é amostra\n",
    "            .when((col(\"calc_ratings_count\") >= 100) & (col(\"calc_ratings_count\") <= 1000), \"Média\")\n",
    "            .otherwise(\"Alta\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Selecionar e organizar colunas finais\n",
    "dim_livros_final = dim_livros_gold.select(\n",
    "    \"book_id\",\n",
    "    \"goodreads_book_id\",\n",
    "    \"title\",\n",
    "    \"authors\",\n",
    "    \"publication_year\",\n",
    "    \"calc_avg_rating\",\n",
    "    \"calc_ratings_count\",\n",
    "    \"popularity_level\",\n",
    "    \"average_rating\" # Mantendo a original para comparação de Data Quality\n",
    ")\n",
    "\n",
    "dim_livros_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.dim_livros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9630e57c-2db2-438d-b961-4e54e7e1ba98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabelas de Tags Gold\nPipeline GOLD finalizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 5. TAGS e BRIDGE (N:N)\n",
    "# -----------------------------------------------------------\n",
    "print(\"tabelas de Tags Gold\")\n",
    "\n",
    "# Dimensão Tag (Apenas o dicionário de tags)\n",
    "tags_silver = spark.table(\"silver.tags\")\n",
    "tags_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.dim_tags\")\n",
    "\n",
    "# Tabela Ponte (Bridge) para ligar Livros <-> Tags\n",
    "# IMPORTANTE: Usamos goodreads_book_id aqui\n",
    "book_tags_silver = spark.table(\"silver.book_tags\")\n",
    "\n",
    "bridge_livros_tags = (\n",
    "    book_tags_silver\n",
    "        .join(tags_silver, on=\"tag_id\", how=\"inner\")\n",
    "        .select(\n",
    "            col(\"goodreads_book_id\"), # Chave de ligação\n",
    "            col(\"tag_id\"),\n",
    "            col(\"tag_name\"),\n",
    "            col(\"count\").alias(\"tag_count\")\n",
    "        )\n",
    ")\n",
    "\n",
    "bridge_livros_tags.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.bridge_livros_tags\")\n",
    "\n",
    "print(\"Pipeline GOLD finalizado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Analise_Exploratoria_engenharia_de_dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}